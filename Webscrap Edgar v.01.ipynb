{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping EDGAR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Extracting ahef links from page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(link):\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Mobile Safari/537.36'}\n",
    "\n",
    "    req = requests.get(link,headers=hdr)\n",
    "    content = req.content\n",
    "  \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CIK=0000036684 is Citi (0000893868)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0000036684&type=&dateb=&owner=exclude&count=1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#req = Request(url)\n",
    "#html = urlopen(req).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = get_data(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = soup.select(\"#documentsbutton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in all_links:\n",
    "    link_list.append(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Archives/edgar/data/36684/000083100121000022/0000831001-21-000022-index.htm'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Extract link for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in link_list:\n",
    "    #html = urlopen(\"https://www.sec.gov\"+url)\n",
    "    html = get_data(\"https://www.sec.gov\"+url)\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    rows = soup.find_all(\"tr\")\n",
    "    for row in rows:\n",
    "        row_td = row.find_all(\"td\")\n",
    "        for file in row_td:\n",
    "            main_links = file.find_all('a')\n",
    "            for data in main_links:\n",
    "                data_list.append(data['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/36684/000083100121000073/0000831001-21-000073-index.htm\n"
     ]
    }
   ],
   "source": [
    "print(\"https://www.sec.gov\"+link_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Separate links with different file extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_htm = []\n",
    "file_xml = []\n",
    "file_html = []\n",
    "file_gif = []\n",
    "file_txt = []\n",
    "file_jpg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.xml'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.splitext(data_list[1])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for extensions in data_list:\n",
    "    case = os.path.splitext(extensions)[1]\n",
    "    if case == '.htm':\n",
    "        file_htm.append(extensions)\n",
    "    elif case == '.html':\n",
    "        file_html.append(extensions)\n",
    "    elif case == '.xml':\n",
    "        file_xml.append(extensions) \n",
    "    elif case == '.gif':\n",
    "        file_gif.append(extensions)\n",
    "    elif case ==  '.txt':\n",
    "        file_txt.append(extensions)\n",
    "    elif case ==  '.jpg':\n",
    "        file_jpg.append(extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Archives/edgar/data/36684/000083100121000073/primary_doc.xml'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_xml[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Converting htm links to text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(os.path.isdir('../txt_files_main')):\n",
    "    print(\"directory already exists\")\n",
    "else:\n",
    "    os.mkdir('../txt_files_main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_no = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/36684/000119312515194885/d928481d40app.htm\n",
      "https://www.sec.gov/Archives/edgar/data/36684/000114420414047144/v385813_40app-a.htm\n",
      "https://www.sec.gov/Archives/edgar/data/36684/000114420411058535/v237497_40app.htm\n",
      "https://www.sec.gov/Archives/edgar/data/36684/000114420410054401/v199373_40appa.htm\n",
      "https://www.sec.gov/Archives/edgar/data/36684/000114420410040425/v192025_40-appa.htm\n",
      "https://www.sec.gov/Archives/edgar/data/36684/000114420410040081/v191550_40-app.htm\n",
      "https://www.sec.gov/Archives/edgar/data/5094/000110465909058519/filename1.htm\n",
      "https://www.sec.gov/Archives/edgar/data/5094/000110465909058494/a09-30662_140appa.htm\n",
      "https://www.sec.gov/Archives/edgar/data/5094/000110465909048493/a09-21695_140appa.htm\n",
      "https://www.sec.gov/Archives/edgar/data/5094/000110465909038382/a09-16054_140appa.htm\n",
      "https://www.sec.gov/Archives/edgar/data/5094/000110465909028708/a09-12150_140app.htm\n",
      "https://www.sec.gov/Archives/edgar/data/36684/000114420409013637/v142686_40-app.htm\n"
     ]
    }
   ],
   "source": [
    "for htmlfiles in file_htm:\n",
    "    html = get_data(\"https://www.sec.gov\" + htmlfiles)\n",
    "    print(\"https://www.sec.gov\" + htmlfiles) # for debugging to see which htm will be saved as text file \n",
    "    soup = bs(html, features = \"html.parser\")\n",
    "    for script in soup([\"script\",\"style\"]):\n",
    "        script.extract()\n",
    "    text = soup.get_text()\n",
    "        # break into lines\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "    chunks = (pharse.strip() for line in lines for pharse in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    f = open(\"../txt_files_main/test\"+str(txt_no)+\".txt\",\"w\",encoding=\"utf-8\")\n",
    "    f.write(text)\n",
    "    txt_no+=1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Converting xml links to text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(os.path.isdir('../txt_files_main_xml')):\n",
    "    print(\"directory already exists\")\n",
    "else:\n",
    "    os.mkdir('../txt_files_main_xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_no = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for htmlfiles in file_xml:\n",
    "    html = get_data(\"https://www.sec.gov\" + htmlfiles)\n",
    "    #print(\"https://www.sec.gov\" + htmlfiles) # for debugging to see which htm will be saved as text file \n",
    "    soup = bs(html, features = \"html.parser\")\n",
    "    for script in soup([\"script\",\"style\"]):\n",
    "        script.extract()\n",
    "    text = soup.get_text()\n",
    "        # break into lines\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "    chunks = (pharse.strip() for line in lines for pharse in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    f = open(\"../txt_files_main_xml/test\"+str(txt_no)+\".txt\",\"w\",encoding=\"utf-8\")\n",
    "    f.write(text)\n",
    "    txt_no+=1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Saving text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(os.path.isdir('../txt_files_main_txt')):\n",
    "    print(\"directory already exists\")\n",
    "else:\n",
    "    os.mkdir('../txt_files_main_txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_no = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for htmlfiles in file_txt:\n",
    "    html = get_data(\"https://www.sec.gov\" + htmlfiles)\n",
    "    #print(\"https://www.sec.gov\" + htmlfiles) # for debugging to see which htm will be saved as text file \n",
    "    soup = bs(html, features = \"html.parser\")\n",
    "    for script in soup([\"script\",\"style\"]):\n",
    "        script.extract()\n",
    "    text = soup.get_text()\n",
    "        # break into lines\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "    chunks = (pharse.strip() for line in lines for pharse in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    f = open(\"../txt_files_main_txt/test\"+str(txt_no)+\".txt\",\"w\",encoding=\"utf-8\")\n",
    "    f.write(text)\n",
    "    txt_no+=1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
